import os
import torch
import secure_torch

print("\n" + "â•" * 70)
print("ğŸ›¡ï¸  SECURE-TORCH: REAL-WORLD THREAT DEMONSTRATION  ğŸ›¡ï¸")
print("â•" * 70 + "\n")

# 1. Provide Context
print("ğŸ¯ GOAL: Show how standard PyTorch loading is vulnerable to RCE ")
print("          (Remote Code Execution) and how secure-torch blocks it.\n")

# 2. Crafting the Malicious Payload
class MaliciousPayload:
    def __reduce__(self):
        # This payload will print a scary message and simulate a system breach
        command = 'echo "ğŸ›‘ BOOM! ğŸ›‘ You just ran malicious code hidden in a model!"'
        # Windows compatibility
        if os.name == 'nt':
            command = 'echo ğŸ›‘ BOOM! ğŸ›‘ You just ran malicious code hidden in a model!'
        return (os.system, (command,))

payload_file = "malicious_weights.bin"
print(f"[*] ğŸ˜ˆ Crafting a malicious model payload: '{payload_file}'...")
torch.save({"weights": MaliciousPayload()}, payload_file)
print("    âœ… Payload successfully injected and saved.\n")


# 3. Scenario 1: Unprotected Load
print("âŒ SCENARIO 1: Unprotected `torch.load()`")
print("-" * 50)
print("[*] â³ Data scientist downloads and loads the model...\n")

try:
    # Danger!
    # The moment torch.load is called, the __reduce__ method from our malicious payload runs!
    torch.load(payload_file, weights_only=False)
except Exception as e:
    pass

print("\n    ğŸš¨ UH OH! The attacker's code just executed on our system!")
print("    The execution happened BEFORE we even opened the weights!\n")


# 4. Scenario 2: Protected Load with secure-torch
print("ğŸ›¡ï¸  SCENARIO 2: Protected `secure_torch.load()`")
print("-" * 50)
print("[*] â³ SecOps engineer loads the same file with `secure-torch`...\n")

try:
    # safe!
    secure_torch.load(payload_file)
    print("    âŒ FAILURE - Model loaded successfully (this shouldn't happen!)")
except Exception as e:
    print(f"    âœ… SUCCESS: Exploit blocked instantly!")
    print(f"    ğŸ”’ Exception Raised: {e.__class__.__name__}: {e}\n")


# 5. Scenario 3: Deep Threat Auditing
print("ğŸ” SCENARIO 3: Threat Intelligence Audit")
print("-" * 50)
print("[*] ğŸ” Inspecting the file contents without executing it...\n")

try:
    _, report = secure_torch.load(payload_file, audit_only=True)
    print(f"    ğŸ“Š Threat Level:      {report.threat_level.name}")
    print(f"    ğŸ§® Score Breakdown:   {report.score_breakdown}")
    
    print("\n    âš ï¸  Warnings Generated:")
    for warn in report.warnings:
         print(f"       - {warn}")
except Exception as e:
    print(f"    âœ… Audit completed... {e}")

print("\n" + "â•" * 70)
print("ğŸ‰ Secure your AI pipeline today! `pip install secure-torch`")
print("â•" * 70 + "\n")

# Cleanup
if os.path.exists(payload_file):
    os.remove(payload_file)
